{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "efad6479-7fb1-40c4-8610-8963f8de307f",
      "metadata": {
        "id": "efad6479-7fb1-40c4-8610-8963f8de307f"
      },
      "source": [
        "# Lab | Agent & Vector store\n",
        "\n",
        "**Change the state union dataset and replicate this lab by updating the prompts accordingly.**\n",
        "\n",
        "One such dataset is the [sonnets.txt](https://github.com/martin-gorner/tensorflow-rnn-shakespeare/blob/master/shakespeare/sonnets.txt) dataset or any other data of your choice from the same git."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b24990",
      "metadata": {
        "id": "68b24990"
      },
      "source": [
        "# Combine agents and vector stores\n",
        "\n",
        "This notebook covers how to combine agents and vector stores. The use case for this is that you've ingested your data into a vector store and want to interact with it in an agentic manner.\n",
        "\n",
        "The recommended method for doing so is to create a `RetrievalQA` and then use that as a tool in the overall agent. Let's take a look at doing this below. You can do this with multiple different vector DBs, and use the agent as a way to route between them. There are two different ways of doing this - you can either let the agent use the vector stores as normal tools, or you can set `return_direct=True` to really just use the agent as a router."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b22020a",
      "metadata": {
        "id": "9b22020a"
      },
      "source": [
        "## Create the vector store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \\\n",
        "    pydantic \\\n",
        "    langchain>=0.1.0 \\\n",
        "    langchain_community \\\n",
        "    langchain_openai \\\n",
        "    chromadb \\\n",
        "    tiktoken\n"
      ],
      "metadata": {
        "id": "CD23PGzOwEtG"
      },
      "id": "CD23PGzOwEtG",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2868cff1-a3ef-426f-b8d7-7fe89047a5b7",
      "metadata": {
        "tags": [],
        "id": "2868cff1-a3ef-426f-b8d7-7fe89047a5b7"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aeff3a2d-b0cc-429f-a34c-4381d71f1a5f",
      "metadata": {
        "tags": [],
        "id": "aeff3a2d-b0cc-429f-a34c-4381d71f1a5f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f16d010b",
      "metadata": {
        "id": "f16d010b"
      },
      "outputs": [],
      "source": [
        "# If you're using colab, run this\n",
        "os.environ['OPENAI_API_KEY'] = \"OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "14331eec-fd46-42e0-b6e7-adaf21824ef7",
      "metadata": {
        "tags": [],
        "id": "14331eec-fd46-42e0-b6e7-adaf21824ef7"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0b7b772b",
      "metadata": {
        "tags": [],
        "id": "0b7b772b"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "relevant_parts = []\n",
        "for p in Path(\".\").absolute().parts:\n",
        "    relevant_parts.append(p)\n",
        "    if relevant_parts[-3:] == [\"langchain\", \"docs\", \"modules\"]:\n",
        "        break\n",
        "doc_path = str(Path(*relevant_parts) / \"state_of_the_union.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f2675861",
      "metadata": {
        "tags": [],
        "id": "f2675861"
      },
      "outputs": [],
      "source": [
        "\n",
        "loader = TextLoader(doc_path)\n",
        "documents = loader.load()\n",
        "\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings, collection_name=\"ai-2024\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bc5403d4",
      "metadata": {
        "tags": [],
        "id": "bc5403d4"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "ai_qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1431cded",
      "metadata": {
        "tags": [],
        "id": "1431cded"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "915d3ff3",
      "metadata": {
        "tags": [],
        "id": "915d3ff3"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\"https://beta.ruff.rs/docs/faq/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "96a2edf8",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96a2edf8",
        "outputId": "76fce847-3de3-42ac-bee9-a14bea39e846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2122, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3187, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1017, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1256, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2321, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "docs = loader.load()\n",
        "ruff_texts = text_splitter.split_documents(docs)\n",
        "ruff_db = Chroma.from_documents(ruff_texts, embeddings, collection_name=\"ruff\")\n",
        "ruff = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=ruff_db.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a6c031",
      "metadata": {
        "id": "c0a6c031"
      },
      "source": [
        "## Create the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "eb142786",
      "metadata": {
        "tags": [],
        "id": "eb142786"
      },
      "outputs": [],
      "source": [
        "# Import things that are needed generically\n",
        "from langchain.agents import AgentType, Tool, initialize_agent\n",
        "from langchain_openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "850bc4e9",
      "metadata": {
        "tags": [],
        "id": "850bc4e9"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"AI 2024 QA System\",\n",
        "        func=ai_qa.run,\n",
        "        description=\"Useful for questions about the AI landscape in 2024. Input should be a full question.\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Ruff QA System\",\n",
        "        func=ruff.run,\n",
        "        description=\"Useful for questions about Ruff (a Python linter). Input should be a full question.\",\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fc47f230",
      "metadata": {
        "tags": [],
        "id": "fc47f230"
      },
      "outputs": [],
      "source": [
        "# Construct the agent. We will use the default agent type here.\n",
        "# See documentation for a full list of options.\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "10ca2db8",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ca2db8",
        "outputId": "6d2659d4-ec6f-46ee-904a-59d29a75e3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m We should consider the potential ethical and societal implications of AI.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: \"What were the main concerns about AI in 2024?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m The main concerns about AI in 2024 were related to surveillance, data privacy, job displacement, and the authenticity of AI-generated content.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m We should also look into any advancements or breakthroughs in AI technology.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: \"What were the main advancements in AI technology in 2024?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Some of the main advancements in AI technology in 2024 included its integration into public systems such as healthcare, education, and transportation, the development of multi-modal AI models, and the launch of open-data initiatives and a Global AI Governance Framework.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m It's important to also consider the global perspective on AI.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: \"What were the main global concerns about AI in 2024?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m The main global concerns about AI in 2024 were related to ethical issues such as surveillance, data privacy, and job displacement. There were also calls for regulations and technologies to verify the authenticity of AI-generated content.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: The main concerns about AI in 2024 were related to ethical and societal implications, advancements in AI technology, and the global perspective on AI.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What were the main concerns about AI in 2024?',\n",
              " 'output': 'The main concerns about AI in 2024 were related to ethical and societal implications, advancements in AI technology, and the global perspective on AI.'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "agent.invoke(\"What were the main concerns about AI in 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4e91b811",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e91b811",
        "outputId": "9bfc228f-8b6e-4c98-9f36-ea38601aa434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m We should use the AI 2024 QA System to get information about President Biden's statements.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: \"What did President Biden say about AI education in 2024?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m President Biden announced increased funding for AI education and workforce transition programs.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m This is helpful information, but let's also check with the Ruff QA System to see if there are any updates on AI education policies.\n",
            "Action: Ruff QA System\n",
            "Action Input: \"What are the latest updates on AI education policies?\"\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m I don't know.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m It seems like the Ruff QA System is not equipped to answer this question. Let's stick with the AI 2024 QA System for now.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: \"What did President Biden say about AI education in 2024?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m President Biden announced increased funding for AI education and workforce transition programs.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m This is the same information we received earlier. It seems like the AI 2024 QA System is the best resource for this question.\n",
            "Final Answer: President Biden announced increased funding for AI education and workforce transition programs in 2024.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What did President Biden say about AI education in 2024?',\n",
              " 'output': 'President Biden announced increased funding for AI education and workforce transition programs in 2024.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "agent.invoke(\"What did President Biden say about AI education in 2024?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "787a9b5e",
      "metadata": {
        "id": "787a9b5e"
      },
      "source": [
        "## Use the Agent solely as a router"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9161ba91",
      "metadata": {
        "id": "9161ba91"
      },
      "source": [
        "You can also set `return_direct=True` if you intend to use the agent as a router and just want to directly return the result of the RetrievalQAChain.\n",
        "\n",
        "Notice that in the above examples the agent did some extra work after querying the RetrievalQAChain. You can avoid that and just return the result directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f59b377e",
      "metadata": {
        "tags": [],
        "id": "f59b377e"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"AI 2024 QA System\",\n",
        "        func=ai_qa.run,\n",
        "        description=\"Answer questions about the AI developments in 2024.\",\n",
        "        return_direct=True,\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Climate QA System\",\n",
        "        func=ruff.run,\n",
        "        description=\"Answer questions about climate change and sustainability.\",\n",
        "        return_direct=True,\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8615707a",
      "metadata": {
        "tags": [],
        "id": "8615707a"
      },
      "outputs": [],
      "source": [
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "36e718a9",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36e718a9",
        "outputId": "2f14df73-490f-4d46-a146-1a978c71597d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m We should use the AI 2024 QA System to answer this question.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: \"What are the global challenges related to AI and climate change in 2024?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What are the global challenges related to AI and climate change in 2024?',\n",
              " 'output': \" I don't know.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "agent.invoke(\"What are the global challenges related to AI and climate change in 2024?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "edfd0a1a",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edfd0a1a",
        "outputId": "f3aea422-7463-45a7-b9ec-53551744fe30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Ruff and Flake8 are both code linters, so I should use the AI 2024 QA System to answer this question.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: \"What makes Ruff faster than Flake8?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What makes Ruff faster than Flake8?', 'output': \" I don't know.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "agent.invoke(\"What makes Ruff faster than Flake8?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49a0cbbe",
      "metadata": {
        "id": "49a0cbbe"
      },
      "source": [
        "## Multi-Hop vector store reasoning\n",
        "\n",
        "Because vector stores are easily usable as tools in agents, it is easy to use answer multi-hop questions that depend on vector stores using the existing agent framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d397a233",
      "metadata": {
        "tags": [],
        "id": "d397a233"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"AI QA System\",\n",
        "        func=ai_qa.run,\n",
        "        description=\"Answer questions about the 2024 State of the Union, especially AI-related topics. Ask full questions.\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Ruff QA System\",\n",
        "        func=ruff.run,\n",
        "        description=\"Answer questions about the Ruff Python linter. Ask clear, specific questions.\",\n",
        "    ),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "06157240",
      "metadata": {
        "tags": [],
        "id": "06157240"
      },
      "outputs": [],
      "source": [
        "# Construct the agent. We will use the default agent type here.\n",
        "# See documentation for a full list of options.\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b492b520",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b492b520",
        "outputId": "b161e49f-239b-42df-d5ee-edb62e7b2ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I am not sure what ruff is or what it does. I will need to do some research to find out.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: \"What is ruff?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the AI 2024 QA System to answer this question.\n",
            "Action: AI 2024 QA System\n",
            "Action Input: Did the president mention {'input': 'What tool does ruff use to run over Jupyter Notebooks?', 'output': \" I don't know.\"} in the state of the union?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Did the president mention {\\'input\\': \\'What tool does ruff use to run over Jupyter Notebooks?\\', \\'output\\': \" I don\\'t know.\"} in the state of the union?',\n",
              " 'output': \" I don't know.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "\n",
        "tool_name = agent.invoke(\"What tool does ruff use to run over Jupyter Notebooks?\")\n",
        "\n",
        "\n",
        "agent.invoke(f\"Did the president mention {tool_name} in the state of the union?\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWwrfbbVGOA"
   },
   "source": [
    "# LangChain Retrieval Agents\n",
    "\n",
    "Conversational agents can struggle with data freshness, knowledge about specific domains, or accessing internal documentation. By coupling agents with retrieval augmentation tools we no longer have these problems.\n",
    "\n",
    "One the other side, using \"naive\" retrieval augmentation without the use of an agent means we will retrieve contexts with *every* query. Again, this isn't always ideal as not every query requires access to external knowledge.\n",
    "\n",
    "Merging these methods gives us the best of both worlds. In this notebook we'll learn how to do this.\n",
    "\n",
    "To begin, we must install the prerequisite libraries that we will be using in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pva9ehKXUpU2",
    "outputId": "3bbcf2dd-1889-412f-d45a-f56945ac4f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: numpy in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.4-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: langchain in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain_openai in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (0.3.12)\n",
      "Requirement already satisfied: langchain-pinecone in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (0.2.5)\n",
      "Requirement already satisfied: pinecone-notebooks in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: filelock in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain) (0.3.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain_openai) (1.72.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: pinecone<7.0.0,>=6.0.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone) (6.0.2)\n",
      "Requirement already satisfied: langchain-tests<1.0.0,>=0.3.7 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain-pinecone) (0.3.17)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: pytest<9,>=7 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.3.5)\n",
      "Requirement already satisfied: pytest-asyncio<1,>=0.20 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.26.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.28.1)\n",
      "Requirement already satisfied: syrupy<5,>=4 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.9.1)\n",
      "Requirement already satisfied: pytest-socket<1,>=0.6.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.7.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone) (2025.1.31)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone) (2.3.0)\n",
      "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: annotated-types>=0.6.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: iniconfig in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/al-husseinabutaleb/Desktop/Ironhack-sda/env/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.3.0)\n",
      "Using cached numpy-2.2.4-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
      "langchain-chroma 0.2.2 requires numpy<2.0.0,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.4 which is incompatible.\n",
      "scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 2.2.4 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets numpy langchain langchain_openai langchain-pinecone pinecone-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a Jupyter notebook or a similar environment, restart the kernel to ensure the changes take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTgrOQziXUto"
   },
   "source": [
    "## Building the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNyRsz0ZXXaq"
   },
   "source": [
    "We start by constructing our knowledge base. We'll use a mostly prepared dataset called **S**tanford **Qu**estion-**A**nswering **D**ataset (SQuAD) hosted on Hugging Face *Datasets*. We download it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laSDMjqQXuj-",
    "outputId": "5272df99-eb4b-4ec2-c513-504e067be2b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('squad', split='train')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8casaLEpX18U"
   },
   "source": [
    "The dataset does contain duplicate contexts, which we can remove like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "JnWZTcJiXzor",
    "outputId": "42659cec-23c3-4349-b007-676da99d834d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                     title  \\\n",
       "0  5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1  5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2  5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3  5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4  5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3  {'text': ['a Marian place of prayer and reflec...  \n",
       "4  {'text': ['a golden statue of the Virgin Mary'...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.to_pandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "er2nM4vsYD4R",
    "outputId": "d58b5640-be25-495a-b90c-585940c851bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5733bf84d058e614000b61be</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>As at most other universities, Notre Dame's st...</td>\n",
       "      <td>When did the Scholastic Magazine of Notre dame...</td>\n",
       "      <td>{'text': ['September 1876'], 'answer_start': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5733bed24776f41900661188</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>The university is the major seat of the Congre...</td>\n",
       "      <td>Where is the headquarters of the Congregation ...</td>\n",
       "      <td>{'text': ['Rome'], 'answer_start': [119]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5733a6424776f41900660f51</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>The College of Engineering was established in ...</td>\n",
       "      <td>How many BS level degrees are offered in the C...</td>\n",
       "      <td>{'text': ['eight'], 'answer_start': [487]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5733a70c4776f41900660f64</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>All of Notre Dame's undergraduate students are...</td>\n",
       "      <td>What entity provides help with the management ...</td>\n",
       "      <td>{'text': ['Learning Resource Center'], 'answer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id                     title  \\\n",
       "0   5733be284776f41900661182  University_of_Notre_Dame   \n",
       "5   5733bf84d058e614000b61be  University_of_Notre_Dame   \n",
       "10  5733bed24776f41900661188  University_of_Notre_Dame   \n",
       "15  5733a6424776f41900660f51  University_of_Notre_Dame   \n",
       "20  5733a70c4776f41900660f64  University_of_Notre_Dame   \n",
       "\n",
       "                                              context  \\\n",
       "0   Architecturally, the school has a Catholic cha...   \n",
       "5   As at most other universities, Notre Dame's st...   \n",
       "10  The university is the major seat of the Congre...   \n",
       "15  The College of Engineering was established in ...   \n",
       "20  All of Notre Dame's undergraduate students are...   \n",
       "\n",
       "                                             question  \\\n",
       "0   To whom did the Virgin Mary allegedly appear i...   \n",
       "5   When did the Scholastic Magazine of Notre dame...   \n",
       "10  Where is the headquarters of the Congregation ...   \n",
       "15  How many BS level degrees are offered in the C...   \n",
       "20  What entity provides help with the management ...   \n",
       "\n",
       "                                              answers  \n",
       "0   {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "5   {'text': ['September 1876'], 'answer_start': [...  \n",
       "10          {'text': ['Rome'], 'answer_start': [119]}  \n",
       "15         {'text': ['eight'], 'answer_start': [487]}  \n",
       "20  {'text': ['Learning Resource Center'], 'answer...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2_Pt7N6Zg2X"
   },
   "source": [
    "### Initialize the Embedding Model and Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGoS84KYZnSK"
   },
   "source": [
    "We'll be using OpenAI's `text-embedding-ada-002` model initialize via LangChain and the Pinecone vector DB. We start by initializing the embedding model, for this we need an [OpenAI API key](https://platform.openai.com/).\n",
    "\n",
    "*(Note that OpenAI is a paid service and so running the remainder of this notebook may incur some small cost)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U57x2_87YSpb",
    "outputId": "cf4c99af-24b7-4bf5-97ea-71091c8fc2ce"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# get API key from top-right dropdown on OpenAI website\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or getpass(\"Enter your OpenAI API key: \")\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQTfOTR6aBRS"
   },
   "source": [
    "Now we create our vector DB to store our vectors. For this we need to get a [free Pinecone API key](https://app.pinecone.io) — the API key can be found in the \"API Keys\" button found in the left navbar of the Pinecone dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index, we set `dimension` equal to to dimensionality of Ada-002 (`1536`), and use a `metric` also compatible with Ada-002 (this can be either `cosine` or `dotproduct`). We also pass our `spec` to index initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3wrG-9yaJel",
    "outputId": "842bf46f-fd0f-4322-8d90-a7fd769b7687"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {'': {'vector_count': 18891}},\n",
       " 'total_vector_count': 18891,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the name for our vector index that will power retrieval\n",
    "index_name = \"langchain-retrieval-agent\"\n",
    "# Get list of existing indexes to avoid recreating one that exists\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# Check if our vector index already exists\n",
    "if index_name not in existing_indexes:\n",
    "    # Create a new Pinecone vector index with appropriate dimensions for OpenAI embeddings\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # OpenAI's text-embedding-ada-002 produces 1536-dimensional unit vectors that are already normalised\n",
    "        metric='dotproduct',  # Dotproduct is faster than cosine for normalized embeddings, since we forgoe the normalisation step\n",
    "        spec=spec  # Configuration for index storage and performance characteristics\n",
    "    )\n",
    "    # Poll until index is ready for use - initialization takes time\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# Connect to our vector index now that we know it exists\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)  # Brief pause to ensure connection is established\n",
    "# Check index statistics - useful to confirm document count and vector dimensions\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD5IGOoLaVx7"
   },
   "source": [
    "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
    "\n",
    "## Indexing\n",
    "\n",
    "We can perform the indexing task using the LangChain vector store object. But for now it is much faster to do it via the Pinecone python client directly. We will do this in batches of `100` or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "321ebcd192fc41e0a7a2ba9c0e7b8556",
      "7c898f43b39545879013df760aa4f6f1",
      "1b5fe655986842a6afd4a2eb2c0f47fc",
      "76872ca737b64616aeacc90f2f7fc655",
      "0679733cce4f4b5eb25f8f56dc7ebe22",
      "71551fc297b54fb4a21d787d117e01a1",
      "b90283522431447b925b226431bb4fa6",
      "a03d372e8b4f456e8663e54c818e0fe9",
      "6fc773ccbc594fddbc1f2f0adaa96186",
      "07b47869f727455fb5ba67e99767c04c",
      "4b2e12a33584489690aaf69f51cd4a3b"
     ]
    },
    "id": "AhDcbRGTaWPi",
    "outputId": "dca5a4d0-5d19-4542-c26c-3073bfa13f2a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0afa38161a4888b0168f506c7387d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "batch_size = 100  # Process documents in chunks to avoid memory issues\n",
    "texts = []\n",
    "metadatas = []\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "   # Calculate end index, ensuring we don't exceed data length\n",
    "   i_end = min(len(data), i+batch_size)\n",
    "   batch = data.iloc[i:i_end]\n",
    "   \n",
    "   # Extract metadata from each document for retrieval context\n",
    "   metadatas = [{\n",
    "       'title': record['title'],\n",
    "       'text': record['context']\n",
    "   } for j, record in batch.iterrows()]\n",
    "   \n",
    "   # Get raw text for embedding generation\n",
    "   documents = batch['context']\n",
    "   \n",
    "   # Convert text to vector embeddings - this is the core of RAG\n",
    "   embeds = embed.embed_documents(documents)\n",
    "   \n",
    "   # Use consistent IDs to enable updates and retrieval\n",
    "   ids = batch['id']\n",
    "   \n",
    "   # Store vectors with their metadata in Pinecone\n",
    "   # zip() combines the parallel arrays into (id, embedding, metadata) tuples\n",
    "   index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDUnLdy1b7G1"
   },
   "source": [
    "We've indexed everything, now we can check the number of vectors in our index like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiccGZKAb_Qo",
    "outputId": "5cafd8c8-771f-46d0-e261-12c9b5b1b058"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {'': {'vector_count': 18891}},\n",
       " 'total_vector_count': 18891,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View index statistics to confirm data ingestion\n",
    "index.describe_index_stats()\n",
    "\n",
    "# Output shows:\n",
    "# - dimension: 1536 (matches text-embedding-ada-002 dimensions)\n",
    "# - index_fullness: 0.0 (plenty of space available)\n",
    "# - metric: dotproduct (efficient similarity search)\n",
    "# - vector_count: 18891 (documents successfully ingested)\n",
    "# - vector_type: dense (standard embedding format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-3oolT5cCR8"
   },
   "source": [
    "## Creating a Vector Store and Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcZ12U06cCH5"
   },
   "source": [
    "Now that we've build our index we can switch back over to LangChain. We start by initializing a vector store using the same index we just built. We do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0MBJ477-cFNw"
   },
   "outputs": [],
   "source": [
    "# Create a vector store abstraction to enable semantic search\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "text_field = \"text\"  # Metadata field containing document content\n",
    "# Connect LangChain's retrieval interfaces to our Pinecone index\n",
    "vectorstore = PineconeVectorStore(\n",
    "   index=index,      # Our Pinecone index\n",
    "   embedding=embed,  # Same embedding model used during indexing\n",
    "   text_key=text_field  # Field to return in search results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K3xRthWcXzW"
   },
   "source": [
    "As in previous examples, we can use the `similarity_search` method to do a pure semantic search (without the generation component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uITMZtzschJF",
    "outputId": "88fae02f-6f82-47d7-a153-21b3e0615709"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='57338724d058e614000b5c9f', metadata={'title': 'University_of_Notre_Dame'}, page_content=\"In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university's traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president.\"),\n",
       " Document(id='5733a6424776f41900660f51', metadata={'title': 'University_of_Notre_Dame'}, page_content='The College of Engineering was established in 1920, however, early courses in civil and mechanical engineering were a part of the College of Science since the 1870s. Today the college, housed in the Fitzpatrick, Cushing, and Stinson-Remick Halls of Engineering, includes five departments of study – aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, and electrical engineering – with eight B.S. degrees offered. Additionally, the college offers five-year dual degree programs with the Colleges of Arts and Letters and of Business awarding additional B.A. and Master of Business Administration (MBA) degrees, respectively.'),\n",
       " Document(id='5733974d4776f41900660e17', metadata={'title': 'University_of_Notre_Dame'}, page_content='Since 2005, Notre Dame has been led by John I. Jenkins, C.S.C., the 17th president of the university. Jenkins took over the position from Malloy on July 1, 2005. In his inaugural address, Jenkins described his goals of making the university a leader in research that recognizes ethics and building the connection between faith and studies. During his tenure, Notre Dame has increased its endowment, enlarged its student body, and undergone many construction projects on campus, including Compton Family Ice Arena, a new architecture hall, additional residence halls, and the Campus Crossroads, a $400m enhancement and expansion of Notre Dame Stadium.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform semantic search to find relevant documents for our query\n",
    "query = \"when was the college of engineering in the University of Notre Dame established?\"\n",
    "vectorstore.similarity_search(\n",
    "   query,  # Question we want to answer\n",
    "   k=3     # Retrieve top 3 most semantically similar documents\n",
    ")\n",
    "# This search uses vector embeddings to find contextually relevant information\n",
    "# even if exact keywords don't match (unlike traditional search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zGF6YsgczqT"
   },
   "source": [
    "Looks like we're getting good results. Let's take a look at how we can begin integrating this into a conversational agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFsIOm73dcOI"
   },
   "source": [
    "## Initializing the Conversational Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMv6TXWkdfNR"
   },
   "source": [
    "Our conversational agent needs a Chat LLM, conversational memory, and a `RetrievalQA` chain to initialize. We create these using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zMRs9Klic5-Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/05g26w8514ldk5czpjvqsgdm0000gn/T/ipykernel_9970/3560879282.py:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  conversational_memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize LLM for generating answers from retrieved context\n",
    "llm = ChatOpenAI(\n",
    "   openai_api_key=OPENAI_API_KEY,\n",
    "   model_name='gpt-3.5-turbo',\n",
    "   temperature=0.0  # Deterministic responses for predictable behavior\n",
    ")\n",
    "\n",
    "# Set up memory to maintain conversation context\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "   memory_key='chat_history',\n",
    "   k=5,  # Store last 5 interactions\n",
    "   return_messages=True  # Return as message objects for LLM chain\n",
    ")\n",
    "\n",
    "# Create RAG pipeline:\n",
    "# 1. Query → 2. Retrieve relevant docs → 3. Generate answer\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "   llm=llm,\n",
    "   chain_type=\"stuff\",  # \"Stuff\" method passes all retrieved docs to LLM at once\n",
    "   retriever=vectorstore.as_retriever()  # Connect to our vector database\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ySfWyZLdboX"
   },
   "source": [
    "Using these we can generate an answer using the `run` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LaYSq0V-dxHw",
    "outputId": "a85730a3-ab15-49b6-8aba-6da774fd3e3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/05g26w8514ldk5czpjvqsgdm0000gn/T/ipykernel_9970/2828950282.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  qa.run(query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The College of Engineering at the University of Notre Dame was established in 1920.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtSXR5RXdyU0"
   },
   "source": [
    "But this isn't yet ready for our conversational agent. For that we need to convert this retrieval chain into a tool. We do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FwCYrS4duqBW"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "# Define tools that the agent can use to solve tasks\n",
    "tools = [\n",
    "   Tool(\n",
    "       name='Knowledge Base',  # Tool identifier\n",
    "       func=qa.run,            # Function to execute (our RAG pipeline)\n",
    "       description=(\n",
    "           'use this tool when answering general knowledge queries to get '\n",
    "           'more information about the topic'\n",
    "       )  # Helps agent decide when to use this tool\n",
    "   )\n",
    "]\n",
    "# Tools enable agents to interact with external systems\n",
    "# This gives our agent access to the RAG system we built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXi_0ipTvM_l"
   },
   "source": [
    "Now we can initialize the agent like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JaKTzPUEvOoy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/05g26w8514ldk5czpjvqsgdm0000gn/T/ipykernel_9970/640423283.py:3: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# Create an LLM agent that can use tools to solve complex tasks\n",
    "agent = initialize_agent(\n",
    "   agent='chat-conversational-react-description',  # Agent type that maintains conversation and explains actions\n",
    "   tools=tools,                                    # Tools the agent can use (our RAG system)\n",
    "   llm=llm,                                        # Language model for reasoning\n",
    "   verbose=True,                                   # Show agent's thought process\n",
    "   max_iterations=3,                               # Prevent infinite loops\n",
    "   early_stopping_method='generate',               # Stop if agent can't make progress\n",
    "   memory=conversational_memory                    # Access to conversation history\n",
    ")\n",
    "# This agent can now use RAG to answer questions about domain-specific content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbXl-AzVvszB"
   },
   "source": [
    "With that our retrieval augmented conversational agent is ready and we can begin using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlxUBWKcvzeP"
   },
   "source": [
    "### Using the Conversational Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZapCP4Pv2kz"
   },
   "source": [
    "To make queries we simply call the `agent` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJoAhy76vzAB",
    "outputId": "31ae7589-e66a-4835-d1d8-555bb59c962c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/05g26w8514ldk5czpjvqsgdm0000gn/T/ipykernel_9970/4024130983.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"Establishment date of the College of Engineering at the University of Notre Dame\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe College of Engineering at the University of Notre Dame was established in 1920.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The College of Engineering at the University of Notre Dame was established in 1920.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'when was the college of engineering in the University of Notre Dame established?',\n",
       " 'chat_history': [],\n",
       " 'output': 'The College of Engineering at the University of Notre Dame was established in 1920.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcMqa9Va2hU6"
   },
   "source": [
    "Looks great, now what if we ask it a non-general knowledge question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85vipqC02deV",
    "outputId": "345c724e-eaea-4a20-9445-99794bc743fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The result of 2 * 7 is 14.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 2 * 7?',\n",
       " 'chat_history': [HumanMessage(content='when was the college of engineering in the University of Notre Dame established?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The College of Engineering at the University of Notre Dame was established in 1920.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The result of 2 * 7 is 14.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"what is 2 * 7?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR_b0IN32rQ9"
   },
   "source": [
    "Perfect, the agent is able to recognize that it doesn't need to refer to it's general knowledge tool for that question. Let's try some more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQeicHTj2pmY",
    "outputId": "9810647d-0846-4ca8-ae2e-1b249ca4ad3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"University of Notre Dame facts\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe University of Notre Dame, located in South Bend, Indiana, is a Catholic research university with a large undergraduate and graduate program. It is known for its strong alumni network, research institutes, and notable landmarks like the Golden Dome and the Basilica. The university offers a variety of degree programs, including a MD-PhD program in collaboration with IU medical School. Notre Dame has a diverse student body representing all 50 states and 100 countries. The university is also recognized for its intramural sports program and annual events like the Bookstore Basketball tournament and the Bengal Bouts tournament.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The University of Notre Dame, located in South Bend, Indiana, is a Catholic research university with a large undergraduate and graduate program. It is known for its strong alumni network, research institutes, and notable landmarks like the Golden Dome and the Basilica. The university offers a variety of degree programs, including a MD-PhD program in collaboration with IU medical School. Notre Dame has a diverse student body representing all 50 states and 100 countries. The university is also recognized for its intramural sports program and annual events like the Bookstore Basketball tournament and the Bengal Bouts tournament.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'can you tell me some facts about the University of Notre Dame?',\n",
       " 'chat_history': [HumanMessage(content='when was the college of engineering in the University of Notre Dame established?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The College of Engineering at the University of Notre Dame was established in 1920.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is 2 * 7?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The result of 2 * 7 is 14.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The University of Notre Dame, located in South Bend, Indiana, is a Catholic research university with a large undergraduate and graduate program. It is known for its strong alumni network, research institutes, and notable landmarks like the Golden Dome and the Basilica. The university offers a variety of degree programs, including a MD-PhD program in collaboration with IU medical School. Notre Dame has a diverse student body representing all 50 states and 100 countries. The university is also recognized for its intramural sports program and annual events like the Bookstore Basketball tournament and the Bengal Bouts tournament.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"can you tell me some facts about the University of Notre Dame?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G93vLXso3B5Z",
    "outputId": "cb345147-5630-4a5a-bb9f-242ad91d9968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The University of Notre Dame is a Catholic research university known for its strong alumni network, research institutes, and notable landmarks. It offers a variety of degree programs and has a diverse student body representing all 50 states and 100 countries.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'can you summarize these facts in two short sentences',\n",
       " 'chat_history': [HumanMessage(content='when was the college of engineering in the University of Notre Dame established?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The College of Engineering at the University of Notre Dame was established in 1920.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is 2 * 7?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The result of 2 * 7 is 14.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='can you tell me some facts about the University of Notre Dame?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The University of Notre Dame, located in South Bend, Indiana, is a Catholic research university with a large undergraduate and graduate program. It is known for its strong alumni network, research institutes, and notable landmarks like the Golden Dome and the Basilica. The university offers a variety of degree programs, including a MD-PhD program in collaboration with IU medical School. Notre Dame has a diverse student body representing all 50 states and 100 countries. The university is also recognized for its intramural sports program and annual events like the Bookstore Basketball tournament and the Bengal Bouts tournament.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The University of Notre Dame is a Catholic research university known for its strong alumni network, research institutes, and notable landmarks. It offers a variety of degree programs and has a diverse student body representing all 50 states and 100 countries.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"can you summarize these facts in two short sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWivmw9F3bCw"
   },
   "source": [
    "Looks great! We're also able to ask questions that refer to previous interactions in the conversation and the agent is able to refer to the conversation history to as a source of information.\n",
    "\n",
    "That's all for this example of building a retrieval augmented conversational agent with OpenAI and Pinecone (the OP stack) and LangChain.\n",
    "\n",
    "Once finished, we delete the Pinecone index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Pa1whr8V3Wfm"
   },
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykg5TYA033yR"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0679733cce4f4b5eb25f8f56dc7ebe22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07b47869f727455fb5ba67e99767c04c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b5fe655986842a6afd4a2eb2c0f47fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a03d372e8b4f456e8663e54c818e0fe9",
      "max": 189,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6fc773ccbc594fddbc1f2f0adaa96186",
      "value": 189
     }
    },
    "321ebcd192fc41e0a7a2ba9c0e7b8556": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c898f43b39545879013df760aa4f6f1",
       "IPY_MODEL_1b5fe655986842a6afd4a2eb2c0f47fc",
       "IPY_MODEL_76872ca737b64616aeacc90f2f7fc655"
      ],
      "layout": "IPY_MODEL_0679733cce4f4b5eb25f8f56dc7ebe22"
     }
    },
    "4b2e12a33584489690aaf69f51cd4a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fc773ccbc594fddbc1f2f0adaa96186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "71551fc297b54fb4a21d787d117e01a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76872ca737b64616aeacc90f2f7fc655": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07b47869f727455fb5ba67e99767c04c",
      "placeholder": "​",
      "style": "IPY_MODEL_4b2e12a33584489690aaf69f51cd4a3b",
      "value": " 189/189 [03:23&lt;00:00,  1.12s/it]"
     }
    },
    "7c898f43b39545879013df760aa4f6f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71551fc297b54fb4a21d787d117e01a1",
      "placeholder": "​",
      "style": "IPY_MODEL_b90283522431447b925b226431bb4fa6",
      "value": "100%"
     }
    },
    "a03d372e8b4f456e8663e54c818e0fe9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b90283522431447b925b226431bb4fa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
